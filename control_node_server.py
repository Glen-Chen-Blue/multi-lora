import os
import time
import uuid
import threading
import asyncio
import httpx
import json
from queue import Queue, Empty
from typing import Dict, List, Deque, Optional, Tuple, Any
from collections import deque, defaultdict
from fastapi import FastAPI, HTTPException, Request
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
import logging

# ============================================================
# Logging & Config
# ============================================================
class EndpointFilter(logging.Filter):
    def filter(self, record):
        msg = record.getMessage()
        return "GET /metrics" not in msg and "GET /status" not in msg

logging.basicConfig(level=logging.INFO)
logging.getLogger("httpx").setLevel(logging.WARNING)
logging.getLogger("uvicorn.access").addFilter(EndpointFilter())

logger = logging.getLogger("ControlNode")

MY_NODE_URL = os.environ.get("MY_NODE_URL", "http://localhost:9000")
EFO_URL = os.environ.get("EFO_URL", "http://localhost:9090")

# åˆå§‹å€™é¸ç¯€é»
ALL_CANDIDATES = [x.strip() for x in os.environ.get("COMPUTE_NODES", "http://127.0.0.1:8001").split(",")]

# Auto-Scaling Config
SCALE_UP_THRESHOLD = int(os.environ.get("SCALE_UP_THRESHOLD", "4"))     
SCALE_COOLDOWN_SEC = float(os.environ.get("SCALE_COOLDOWN_SEC", "5.0"))
MIN_NODES = 1

# Merge Trigger Config
QMIN_MULT = int(os.environ.get("QMIN_MULT", "4")) # Queue > 4 * NodeCount æ™‚è§¸ç™¼ Merge

# ============================================================
# State
# ============================================================
app = FastAPI(title="Control Node with Affinity")
lock = threading.Lock()

# Resource Pools
active_node_urls = []   
standby_node_urls = []  

if ALL_CANDIDATES:
    active_node_urls.append(ALL_CANDIDATES[0])        
    standby_node_urls.extend(ALL_CANDIDATES[1:])      

# Nodes Management
nodes: Dict[str, Dict[str, Any]] = {}

adapter_queues = defaultdict(deque) 
stream_queues = {}
merged_assignment = {} 

my_allowed_adapters = []

# [New] Affinity & Minimal Set from EFO
affinity_table = {} 
minimal_set = []

# æ³¨æ„ï¼šé€™è£¡ä¸å»ºç«‹å…¨åŸŸ AsyncClient ç”¨æ–¼èƒŒæ™¯ä»»å‹™ï¼Œé¿å… Event Loop è¡çª
wakeup = threading.Event()
last_scale_action_ts = 0.0

class AddRequest(BaseModel):
    prompt: str
    adapter_id: str
    max_new_tokens: int = 128

class ConfigUpdate(BaseModel):
    assigned_adapters: List[str]
    affinity_table: Dict[str, List[str]]
    minimal_set: List[str]

# ============================================================
# Helpers
# ============================================================
def _ensure_stream(rid):
    with lock:
        if rid not in stream_queues:
            stream_queues[rid] = (Queue(), time.time())

def _push_data(rid, data):
    with lock:
        if rid in stream_queues: stream_queues[rid][0].put(data)

def _finish_stream(rid):
    with lock:
        if rid in stream_queues: stream_queues[rid][0].put(None)

def _http_post_bg(url, path, payload):
    """
    [MODIFIED] åŠ å…¥é‡è©¦æ©Ÿåˆ¶ï¼Œè§£æ±º Compute Node å•Ÿå‹•è¼ƒæ…¢å°è‡´é€£ç·šè¢«æ‹’çš„å•é¡Œã€‚
    """
    def run():
        # å˜—è©¦ 30 æ¬¡ï¼Œæ¯æ¬¡é–“éš” 2 ç§’ï¼Œå…±ç­‰å¾… 60 ç§’
        max_retries = 30
        for i in range(max_retries):
            try:
                r = httpx.post(f"{url}{path}", json=payload, timeout=5.0)
                if r.status_code == 200:
                    logger.info(f"âœ… Successfully synced to {url}")
                    return
                else:
                    logger.warning(f"âš ï¸ Sync to {url} returned {r.status_code}. Retrying...")
            except Exception as e:
                # åªæœ‰åœ¨å‰å¹¾æ¬¡å¤±æ•—æ™‚å°å‡º Logï¼Œé¿å…æ´—ç‰ˆ
                if i < 3 or i % 10 == 0:
                    logger.info(f"â³ Waiting for {url} to become available... (Attempt {i+1}/{max_retries})")
            
            time.sleep(2.0)
        
        logger.error(f"âŒ Failed to post to {url}{path} after {max_retries} retries.")

    threading.Thread(target=run, daemon=True).start()

def sync_compute_nodes_adapters():
    """
    [NEW] å°‡ç•¶å‰çš„ my_allowed_adapters åŒæ­¥çµ¦æ‰€æœ‰æ´»èºçš„ Compute Nodes
    """
    logger.info(f"ğŸ”„ Syncing adapters to Compute Nodes: {my_allowed_adapters}")
    with lock:
        targets = list(active_node_urls)
    
    for url in targets:
        _http_post_bg(url, "/sync_adapters", {"adapters": my_allowed_adapters})

# ============================================================
# Node State Helpers (Affinity Aware)
# ============================================================
def _update_node_metrics(url, metrics):
    with lock:
        if url not in nodes:
            nodes[url] = {"mode": "NORMAL", "target": None, "metrics": None, "last_seen": 0, "merged_at": 0}
        nodes[url]["metrics"] = metrics
        nodes[url]["last_seen"] = time.time()

def _get_healthy_active_nodes():
    now = time.time()
    res = []
    with lock:
        for url in active_node_urls:
            info = nodes.get(url)
            if info and info.get("metrics") and (now - info["last_seen"] < 5.0):
                res.append(url)
    return res

def _node_can_accept(url, adapter_id):
    """
    æ“´å±•åŸæœ‰çš„æª¢æŸ¥é‚è¼¯ï¼ŒåŠ å…¥èªæ„è¦ªå’ŒåŠ›åˆ¤æ–· (Fuzzy Matching)ã€‚
    """
    with lock:
        info = nodes.get(url)
        if not info or not info.get("metrics"): return False
        
        mode = info["mode"]
        target = info["target"]
        m = info["metrics"]
        
        running = m["load"]["running_batch"]
        max_bs = m["capacity"]["max_batch_size"]
        if running >= max_bs: return False
        
        if mode == "DRAINING": return False
        
        actual_merged = m["lora_state"]["merged_adapter"]
        
        # 0. å¦‚æœç¯€é»è¢«é–å®šåœ¨ MERGED æ¨¡å¼ä½†é‚„æ²’å®Œæˆ Mergeï¼Œæª¢æŸ¥ Target
        if mode == "MERGED" and target != adapter_id:
             substitutes = affinity_table.get(adapter_id, [])
             if target not in substitutes:
                 return False

        # 1. ç²¾ç¢ºåŒ¹é… (Exact Match)
        if actual_merged == adapter_id: return True
        
        # 2. èªæ„è¦ªå’ŒåŠ›åŒ¹é… (Fuzzy Match)
        substitutes = affinity_table.get(adapter_id, [])
        if actual_merged and (actual_merged in substitutes):
            return True
            
        # 3. ç¯€é»æœª Merge ä¸”ç‚º Normal æ¨¡å¼ (å¯ä»¥è‡ªç”±åŠ è¼‰)
        if not actual_merged and mode == "NORMAL":
             return True
        
        return False

# ============================================================
# Scaling & Merging Logic
# ============================================================
def _check_autoscaling():
    global last_scale_action_ts
    now = time.time()
    if (now - last_scale_action_ts) < SCALE_COOLDOWN_SEC: return

    with lock:
        q_total = sum(len(q) for q in adapter_queues.values())
        n_active = len(active_node_urls)
        n_standby = len(standby_node_urls)
    
    if n_standby > 0 and q_total > (SCALE_UP_THRESHOLD * n_active):
        with lock:
            if standby_node_urls:
                new_node = standby_node_urls.pop(0)
                active_node_urls.append(new_node)
                last_scale_action_ts = now
                logger.info(f"ğŸš€ [AutoScaler] Scale UP! Activated: {new_node}")
                
                # [ADDED] æ–°ç¯€é»åŠ å…¥æ™‚ï¼Œç«‹åˆ»åŒæ­¥ Adapter å…è¨±æ¸…å–®
                _http_post_bg(new_node, "/sync_adapters", {"adapters": my_allowed_adapters})
        return

    if n_active > MIN_NODES and q_total == 0:
        candidate = None
        with lock:
            for i in range(len(active_node_urls) - 1, MIN_NODES - 1, -1):
                url = active_node_urls[i]
                info = nodes.get(url)
                if info and info.get("mode") == "NORMAL" and info["metrics"].get("idle") is True:
                     candidate = url
                     del active_node_urls[i]
                     standby_node_urls.insert(0, url)
                     break
        if candidate:
            last_scale_action_ts = now
            logger.info(f"ğŸ’¤ [AutoScaler] Scale DOWN! Deactivated: {candidate}")

def _maybe_trigger_merge():
    healthy_urls = _get_healthy_active_nodes()
    N = len(healthy_urls)
    if N == 0: return

    with lock:
        # å› ç‚ºæˆ‘å€‘åœ¨ send_request å°±å·²ç¶“åˆä½µäº† IDï¼Œé€™è£¡çš„ counts å°±æ˜¯å·²ç¶“æ­¸é¡éçš„
        counts = {a: len(q) for a, q in adapter_queues.items() if len(q) > 0}
    
    Q = sum(counts.values())
    if Q < (QMIN_MULT * N): return 

    demand_threshold = Q / N

    hot_candidates = []
    with lock:
        assigned_adapters = set(merged_assignment.keys())
        for a, c in counts.items():
            if a not in assigned_adapters:
                if c > demand_threshold:
                    hot_candidates.append((c, a))
    
    if not hot_candidates: return
    hot_candidates.sort(reverse=True)
    _, target_adapter = hot_candidates[0]

    target_node = None
    with lock:
        best_score = -1
        for url in healthy_urls:
            info = nodes.get(url)
            if info["mode"] != "NORMAL": continue
            
            m = info["metrics"]
            running_adapters = m["lora_state"]["running_adapters"]
            has_adapter = 1 if target_adapter in running_adapters else 0
            load = m["load"]["running_batch"]
            
            score = (has_adapter * 100) - load
            if score > best_score:
                best_score = score
                target_node = url
        
        if target_node:
            nodes[target_node]["mode"] = "DRAINING"
            nodes[target_node]["target"] = target_adapter
            logger.info(f"ğŸ”’ [Merge] Locking {target_node} to DRAIN for {target_adapter} (Queue: {counts[target_adapter]}, Threshold: {demand_threshold:.1f})")

def _maybe_finalize_drains():
    with lock:
        candidates = []
        for url, info in nodes.items():
            if info.get("mode") == "DRAINING" and info.get("metrics"):
                candidates.append((url, info["target"], info["metrics"]["idle"]))
    
    for url, target, is_idle in candidates:
        if is_idle:
            logger.info(f"ğŸ”— [Merge] Node {url} is idle. Sending MERGE {target}...")
            try:
                httpx.post(f"{url}/unmerge", json={"force": True}, timeout=2)
                httpx.post(f"{url}/merge", json={"adapter_id": target, "force": True}, timeout=2)
                
                with lock:
                    nodes[url]["mode"] = "MERGED"
                    nodes[url]["merged_at"] = time.time()
                    merged_assignment[target] = url
                logger.info(f"âœ… [Merge] Node {url} is now MERGED for {target}")
            except Exception as e:
                logger.error(f"âŒ [Merge] Failed to finalize merge on {url}: {e}")

def _maybe_revert_merges():
    with lock:
        revert_list = []
        for adapter, url in merged_assignment.items():
            if len(adapter_queues[adapter]) > 0: continue
            
            info = nodes.get(url)
            if info and info.get("metrics") and info["metrics"]["idle"]:
                merged_at = info.get("merged_at", 0)
                if time.time() - merged_at < 30.0:
                    continue 

                revert_list.append((adapter, url))
    
    for adapter, url in revert_list:
        logger.info(f"ğŸ”“ [Merge] Reverting MERGE for {adapter} on {url} (Idle)")
        _http_post_json_bg(url, "/unmerge", {"force": False})
        with lock:
            if url in nodes:
                nodes[url]["mode"] = "NORMAL"
                nodes[url]["target"] = None
                nodes[url]["merged_at"] = 0
            if merged_assignment.get(adapter) == url:
                del merged_assignment[adapter]

def _http_post_json_bg(url, path, json_data):
    threading.Thread(target=lambda: httpx.post(f"{url}{path}", json=json_data), daemon=True).start()

# ============================================================
# Background Tasks
# ============================================================
def efo_heartbeat():
    """
    [Modified] Heartbeat now just pings EFO. Config comes via Push (/update_config).
    """
    # 1. Loop until registered
    while True:
        try:
            logger.info("Registering to EFO...")
            r = httpx.post(f"{EFO_URL}/register_node", json={"control_node_url": MY_NODE_URL}, timeout=5)
            if r.status_code == 200:
                logger.info("âœ… Registered to EFO.")
                break
        except Exception as e:
            logger.warning(f"EFO registration failed: {e}. Retrying...")
            time.sleep(5)
    
    # 2. Simple Heartbeat
    while True:
        time.sleep(10)
        try:
            httpx.post(f"{EFO_URL}/heartbeat", json={"control_node_url": MY_NODE_URL}, timeout=2)
        except Exception:
            pass

def compute_poller():
    while True:
        with lock: targets = list(active_node_urls)
        for url in targets:
            try:
                r = httpx.get(f"{url}/metrics", timeout=1)
                _update_node_metrics(url, r.json())
            except: 
                pass 
        wakeup.set()
        time.sleep(0.5)

def scheduler():
    while True:
        wakeup.wait()
        
        _check_autoscaling()
        _maybe_trigger_merge()
        _maybe_finalize_drains()
        _maybe_revert_merges()

        with lock:
            merged_queues = [a for a in merged_assignment.keys() if adapter_queues[a]]
            normal_queues = [a for a in adapter_queues if adapter_queues[a] and a not in merged_assignment]
            
        did_work = False

        # 2a. Dispatch Merged Queues (Priority)
        for aid in merged_queues:
            target_node = None
            with lock: target_node = merged_assignment.get(aid)
            
            if target_node and _node_can_accept(target_node, aid):
                req = None
                with lock:
                    if adapter_queues[aid]: req = adapter_queues[aid].popleft()
                
                if req:
                    _dispatch_to_compute(target_node, req)
                    did_work = True

        # 2b. Dispatch Normal Queues
        for aid in normal_queues:
            target_node = None
            healthy = _get_healthy_active_nodes()
            
            for url in healthy:
                if _node_can_accept(url, aid):
                    target_node = url
                    break
            
            if target_node:
                req = None
                with lock:
                    if adapter_queues[aid]: req = adapter_queues[aid].popleft()
                if req:
                    # æ³¨æ„: é€™è£¡å·²ç¶“åœ¨ send_request åšéä¸€æ¬¡ ID Rewrite
                    # æ‰€ä»¥ req['adapter_id'] å·²ç¶“æ˜¯ Compute Node æ“æœ‰çš„ ID (ä¾‹å¦‚ '1')
                    # ä½†å¦‚æœç›®æ¨™ç¯€é»å‰›å¥½æ˜¯è¢« Merge åœ¨æŸå€‹ç›¸å®¹çš„ adapter ä¸Š (ä¾‹å¦‚ä¹Ÿ Merge æˆäº† '1' æˆ–è€…æ˜¯ '5'?)
                    # ä¸€èˆ¬æƒ…æ³ä¸‹ç›´æ¥é€å‡ºå³å¯ã€‚
                    
                    # å†æ¬¡ç¢ºèª: å¦‚æœç›®æ¨™ç¯€é»æ˜¯è¢«é–å®šåœ¨æŸå€‹ merged adapterï¼Œä¸”èˆ‡ç•¶å‰ req ç›¸å®¹ï¼Œ
                    # æˆ‘å€‘è¦ç¢ºä¿é€å‡ºçš„ ID æ˜¯é‚£å€‹ merged IDã€‚
                    
                    target_adapter_to_use = req["adapter_id"]
                    
                    with lock:
                        info = nodes.get(target_node)
                        if info and info.get("metrics"):
                            merged = info["metrics"]["lora_state"].get("merged_adapter")
                            if merged and merged != req["adapter_id"]:
                                substitutes = affinity_table.get(req["adapter_id"], [])
                                if merged in substitutes:
                                    logger.info(f"ğŸ”„ [Scheduler] Swapping {req['adapter_id']} -> {merged} for dispatch to {target_node}")
                                    target_adapter_to_use = merged
                    
                    req_to_send = req.copy()
                    req_to_send["adapter_id"] = target_adapter_to_use
                    
                    _dispatch_to_compute(target_node, req_to_send)
                    did_work = True

        if not did_work:
             time.sleep(0.02)

        with lock:
            if not any(adapter_queues.values()): wakeup.clear()

def reaper():
    while True:
        time.sleep(5)
        now = time.time()
        with lock:
            to_del = [rid for rid, (q, ts) in stream_queues.items() if now - ts > 60]
            for rid in to_del: del stream_queues[rid]

threading.Thread(target=efo_heartbeat, daemon=True).start()
threading.Thread(target=compute_poller, daemon=True).start()
threading.Thread(target=scheduler, daemon=True).start()
threading.Thread(target=reaper, daemon=True).start()

# ============================================================
# Proxy Helper
# ============================================================
def _proxy_to_efo(req_id, prompt, adapter, tokens):
    async def run():
        async with httpx.AsyncClient(timeout=None) as client:
            try:
                async with client.stream("POST", f"{EFO_URL}/relay_request", 
                                         json={"prompt": prompt, "adapter_id": adapter, "max_new_tokens": tokens}) as r:
                    async for line in r.aiter_lines():
                        if line and line.startswith("data:"):
                            content = line[len("data:"):].rstrip("\n")
                            if content: _push_data(req_id, content)
            except Exception as e:
                _push_data(req_id, json.dumps(f"[Error: {e}]"))
            finally:
                _finish_stream(req_id)
    threading.Thread(target=lambda: asyncio.run(run()), daemon=True).start()

def _dispatch_to_compute(url, req):
    async def run():
        async with httpx.AsyncClient(timeout=None) as client:
            try:
                payload = {
                    "prompt": req["prompt"], 
                    "adapter_id": req["adapter_id"],
                    "max_new_tokens": req["max_new_tokens"]
                }
                async with client.stream("POST", f"{url}/add_request", json=payload) as r:
                    if r.status_code != 200:
                        logger.error(f"Compute node {url} rejected request {req['rid']} with {r.status_code}")
                        _push_data(req["rid"], json.dumps(f"[ERROR] Compute node returned {r.status_code}"))
                        return

                    async for line in r.aiter_lines():
                        if line and line.startswith("data:"):
                            content = line[len("data:"):].rstrip("\n")
                            if content and content != "[DONE]": 
                                _push_data(req["rid"], content)
            except Exception as e:
                logger.error(f"Dispatch error to {url}: {e}")
                _push_data(req["rid"], json.dumps(f"[ERROR] Dispatch failed: {e}"))
            finally:
                _finish_stream(req["rid"])
    threading.Thread(target=lambda: asyncio.run(run()), daemon=True).start()

# ============================================================
# API
# ============================================================
@app.post("/update_config")
def update_config(cfg: ConfigUpdate):
    """
    [NEW] æ¥æ”¶ EFO çš„å»£æ’­é…ç½®
    """
    global my_allowed_adapters, affinity_table, minimal_set
    
    changed = (set(my_allowed_adapters) != set(cfg.assigned_adapters))
    
    with lock:
        my_allowed_adapters = cfg.assigned_adapters
        affinity_table = cfg.affinity_table
        minimal_set = cfg.minimal_set
    
    logger.info(f"ğŸ“¥ Received config update from EFO. Assigned: {len(my_allowed_adapters)} adapters.")
    
    # å¦‚æœåˆ†é…çš„ Adapter è®Šäº†ï¼Œé€šçŸ¥ Compute Nodes é‡æ–°åŠ è¼‰
    if changed:
        sync_compute_nodes_adapters()

    return {"status": "updated"}

@app.post("/send_request")
def send_request(req: AddRequest):
    rid = str(uuid.uuid4())
    _ensure_stream(rid)
    
    is_local = False
    # [NEW] ç”¨ä¾†å­˜å„²æœ€çµ‚è¦ä½¿ç”¨çš„ ID (å¯èƒ½æ˜¯åŸå§‹ IDï¼Œä¹Ÿå¯èƒ½æ˜¯æ›¿ä»£å“ ID)
    final_adapter_id = req.adapter_id 

    with lock:
        # Check 1: æœ¬åœ°ç›´æ¥æœ‰ (Exact Match)
        if not my_allowed_adapters or req.adapter_id in my_allowed_adapters:
            is_local = True
        
        # Check 2: æœ¬åœ°æœ‰æ›¿ä»£å“ (Affinity Match in Allowed List)
        # å¦‚æœæˆ‘æ²’æœ‰é€™å€‹ Adapterï¼Œä½†æˆ‘æœ‰å®ƒçš„ Expert (æ›¿ä»£å“) ä¸” Expert åœ¨å…è¨±æ¸…å–®ä¸­ -> æˆ‘å¯ä»¥è™•ç†
        if not is_local:
             substitutes = affinity_table.get(req.adapter_id, [])
             for sub in substitutes:
                 if sub in my_allowed_adapters:
                     is_local = True
                     final_adapter_id = sub # [REWRITE] æ”¹å¯«ç‚ºæ›¿ä»£å“ ID
                     break
        
        # Check 3: Affinity Match in Merged State
        # æª¢æŸ¥æ˜¯å¦æœ‰ç¯€é»å·²ç¶“ Merge äº†æŸå€‹æ›¿ä»£å“
        if not is_local:
             substitutes = affinity_table.get(req.adapter_id, [])
             for url in active_node_urls:
                 info = nodes.get(url)
                 if info and info.get("metrics"):
                     merged = info["metrics"]["lora_state"]["merged_adapter"]
                     # å¦‚æœæŸå€‹ç¯€é» Merge äº†æˆ‘çš„æ›¿ä»£å“ï¼Œé‚£ä¹Ÿå¯ä»¥é€éå»
                     if merged and merged in substitutes:
                         is_local = True
                         # æ³¨æ„ï¼šé€™è£¡ä¸æ”¹å¯« final_adapter_idï¼Œ
                         # å› ç‚º scheduler æœƒå†åšä¸€æ¬¡é‡å° Merged Node çš„æª¢æŸ¥ä¸¦æ”¹å¯«
                         # æˆ–è€…æˆ‘å€‘ä¹Ÿå¯ä»¥åœ¨é€™è£¡æ”¹å¯«ï¼Œä½†ç‚ºäº†é‚è¼¯ä¸€è‡´æ€§ï¼Œè®“ scheduler è™•ç†å‹•æ…‹çš„ merged ç‹€æ…‹æ¯”è¼ƒå¥½
                         break
    
    if is_local:
        with lock:
            # [MODIFIED] ä½¿ç”¨ final_adapter_id å…¥éšŠåˆ—
            # é€™æ¨£çµ¦ 5 çš„è«‹æ±‚å°±æœƒé€²å…¥ '1' çš„éšŠåˆ—ï¼Œè¨ˆæ•¸æœƒåˆä½µï¼ŒMerge ä¹Ÿæœƒæ­£ç¢ºè§¸ç™¼ '1'
            adapter_queues[final_adapter_id].append({
                "rid": rid, 
                "prompt": req.prompt, 
                "adapter_id": final_adapter_id, # [IMPORTANT] ä½¿ç”¨æ”¹å¯«å¾Œçš„ ID
                "max_new_tokens": req.max_new_tokens
            })
            wakeup.set()
    else:
        # Proxy é‚è¼¯ç¶­æŒåŸæ¨£ (é€åŸå§‹ ID çµ¦ EFO é‡æ–°åˆ†é…)
        _proxy_to_efo(rid, req.prompt, req.adapter_id, req.max_new_tokens)
        
    return {"request_id": rid}

@app.get("/stream/{request_id}")
async def stream(request_id: str, request: Request):
    with lock: item = stream_queues.get(request_id)
    if not item: raise HTTPException(404, "Not found")
    q, _ = item
    
    async def gen():
        yield "event: open\ndata: ok\n\n"
        while True:
            if await request.is_disconnected(): break
            try:
                data = q.get_nowait()
                if data is None:
                    yield "event: end\ndata: [DONE]\n\n"
                    break
                yield f"data: {data}\n\n"
            except Empty:
                await asyncio.sleep(0.01)
        with lock: stream_queues.pop(request_id, None)
    
    return StreamingResponse(gen(), media_type="text/event-stream")

@app.get("/status")
def status():
    with lock:
        return {
            "node_type": "CONTROL_NODE",
            "allowed": my_allowed_adapters,
            "affinity_data": {
                "table_size": len(affinity_table),
                "minimal_set": minimal_set
            },
            "active_nodes": active_node_urls,
            "merged": merged_assignment,
            "queues": {k: len(v) for k, v in adapter_queues.items()},
            "node_details": {u: {"mode": i.get("mode"), "target": i.get("target"), "load": i.get("metrics", {}).get("load")} for u, i in nodes.items()}
        }

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=9000)