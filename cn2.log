/home/glenchen/miniconda3/envs/myenv/lib/python3.10/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
INFO:     Started server process [3381342]
INFO:     Waiting for application startup.
[2026-01-12 11:54:01] INFO compute_node_server.py:103: Initializing Compute Node cn-2...
The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
üöÄ [Engine] Initializing vLLM with model: unsloth/Meta-Llama-3.1-8B...
‚öôÔ∏è  Config: slots=4, max_batch=16
üß†  Memory: max_model_len=4096, gpu_util=0.9
INFO 01-12 11:54:02 [model.py:514] Resolved architecture: LlamaForCausalLM
INFO 01-12 11:54:02 [model.py:1661] Using max model len 4096
INFO 01-12 11:54:02 [scheduler.py:230] Chunked prefill is enabled with max_num_batched_tokens=2048.
WARNING 01-12 11:54:03 [system_utils.py:136] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/usage/troubleshooting.html#python-multiprocessing for more information. Reasons: CUDA is initialized
/home/glenchen/miniconda3/envs/myenv/lib/python3.10/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[0;36m(EngineCore_DP0 pid=3381557)[0;0m INFO 01-12 11:54:06 [core.py:93] Initializing a V1 LLM engine (v0.13.0) with config: model='unsloth/Meta-Llama-3.1-8B', speculative_config=None, tokenizer='unsloth/Meta-Llama-3.1-8B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False), seed=0, served_model_name=unsloth/Meta-Llama-3.1-8B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [2048], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 32, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False}, 'local_cache_dir': None}
[0;36m(EngineCore_DP0 pid=3381557)[0;0m INFO 01-12 11:54:06 [parallel_state.py:1203] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://192.168.60.215:59231 backend=nccl
[0;36m(EngineCore_DP0 pid=3381557)[0;0m INFO 01-12 11:54:06 [parallel_state.py:1411] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
[0;36m(EngineCore_DP0 pid=3381557)[0;0m INFO 01-12 11:54:07 [gpu_model_runner.py:3562] Starting to load model unsloth/Meta-Llama-3.1-8B...
[0;36m(EngineCore_DP0 pid=3381557)[0;0m /home/glenchen/miniconda3/envs/myenv/lib/python3.10/site-packages/tvm_ffi/_optional_torch_c_dlpack.py:174: UserWarning: Failed to JIT torch c dlpack extension, EnvTensorAllocator will not be enabled.
[0;36m(EngineCore_DP0 pid=3381557)[0;0m We recommend installing via `pip install torch-c-dlpack-ext`
[0;36m(EngineCore_DP0 pid=3381557)[0;0m   warnings.warn(
[0;36m(EngineCore_DP0 pid=3381557)[0;0m INFO 01-12 11:54:08 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
[0;36m(EngineCore_DP0 pid=3381557)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
[0;36m(EngineCore_DP0 pid=3381557)[0;0m Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:00,  9.11it/s]
[0;36m(EngineCore_DP0 pid=3381557)[0;0m Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:00<00:00,  3.33it/s]
[0;36m(EngineCore_DP0 pid=3381557)[0;0m Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:01<00:00,  2.42it/s]
[0;36m(EngineCore_DP0 pid=3381557)[0;0m Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:01<00:00,  2.02it/s]
[0;36m(EngineCore_DP0 pid=3381557)[0;0m Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:01<00:00,  2.34it/s]
[0;36m(EngineCore_DP0 pid=3381557)[0;0m 
[0;36m(EngineCore_DP0 pid=3381557)[0;0m INFO 01-12 11:54:11 [default_loader.py:308] Loading weights took 1.75 seconds
[0;36m(EngineCore_DP0 pid=3381557)[0;0m INFO 01-12 11:54:11 [punica_selector.py:20] Using PunicaWrapperGPU.
[0;36m(EngineCore_DP0 pid=3381557)[0;0m INFO 01-12 11:54:11 [gpu_model_runner.py:3659] Model loading took 16.3955 GiB memory and 3.778963 seconds
[0;36m(EngineCore_DP0 pid=3381557)[0;0m INFO 01-12 11:54:15 [backends.py:643] Using cache directory: /home/glenchen/.cache/vllm/torch_compile_cache/5f2b7d1d4a/rank_0_0/backbone for vLLM's torch.compile
[0;36m(EngineCore_DP0 pid=3381557)[0;0m INFO 01-12 11:54:15 [backends.py:703] Dynamo bytecode transform time: 4.19 s
[0;36m(EngineCore_DP0 pid=3381557)[0;0m INFO 01-12 11:54:17 [backends.py:261] Cache the graph of compile range (1, 2048) for later use
[0;36m(EngineCore_DP0 pid=3381557)[0;0m INFO 01-12 11:54:19 [backends.py:278] Compiling a graph for compile range (1, 2048) takes 1.76 s
[0;36m(EngineCore_DP0 pid=3381557)[0;0m INFO 01-12 11:54:19 [monitor.py:34] torch.compile takes 5.95 s in total
[0;36m(EngineCore_DP0 pid=3381557)[0;0m INFO 01-12 11:54:20 [gpu_worker.py:375] Available KV cache memory: 3.17 GiB
[0;36m(EngineCore_DP0 pid=3381557)[0;0m INFO 01-12 11:54:20 [kv_cache_utils.py:1291] GPU KV cache size: 25,984 tokens
[0;36m(EngineCore_DP0 pid=3381557)[0;0m INFO 01-12 11:54:20 [kv_cache_utils.py:1296] Maximum concurrency for 4,096 tokens per request: 6.34x
[0;36m(EngineCore_DP0 pid=3381557)[0;0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/14 [00:00<?, ?it/s][0;36m(EngineCore_DP0 pid=3381557)[0;0m WARNING 01-12 11:54:20 [utils.py:250] Using default LoRA kernel configs
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   7%|‚ñã         | 1/14 [00:01<00:22,  1.76s/it]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  21%|‚ñà‚ñà‚ñè       | 3/14 [00:03<00:10,  1.08it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 6/14 [00:03<00:03,  2.64it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 9/14 [00:03<00:01,  4.55it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 12/14 [00:03<00:00,  6.88it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 14/14 [00:05<00:00,  2.78it/s]
[0;36m(EngineCore_DP0 pid=3381557)[0;0m Capturing CUDA graphs (decode, FULL):   0%|          | 0/10 [00:00<?, ?it/s]Capturing CUDA graphs (decode, FULL):  10%|‚ñà         | 1/10 [00:00<00:06,  1.36it/s]Capturing CUDA graphs (decode, FULL):  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 5/10 [00:00<00:00,  7.21it/s]Capturing CUDA graphs (decode, FULL):  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 9/10 [00:01<00:00, 12.36it/s]Capturing CUDA graphs (decode, FULL): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:01<00:00,  9.70it/s]
[0;36m(EngineCore_DP0 pid=3381557)[0;0m INFO 01-12 11:54:26 [gpu_model_runner.py:4587] Graph capturing finished in 7 secs, took 0.19 GiB
[0;36m(EngineCore_DP0 pid=3381557)[0;0m INFO 01-12 11:54:26 [core.py:259] init engine (profile, create kv cache, warmup model) took 15.47 seconds
[2026-01-12 11:54:28] INFO compute_node_server.py:82: üöÄ Engine loop started (vLLM).
INFO:     Application startup complete.
INFO:     Uvicorn running on http://0.0.0.0:8002 (Press CTRL+C to quit)
INFO:     Shutting down
[rank0]:[W112 11:55:03.280065077 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
INFO:     Waiting for application shutdown.
[2026-01-12 11:55:04] INFO compute_node_server.py:120: Shutting down...
INFO:     Application shutdown complete.
INFO:     Finished server process [3381342]
